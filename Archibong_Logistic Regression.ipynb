{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9630308a",
   "metadata": {},
   "source": [
    "# Micro Earthquake Prediction Using Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c9653",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#lib\">Python Libraries<a/></li>\n",
    "<li><a href=\"#aim\">Aim & Objectives<a/></li>\n",
    "<li><a href=\"#explore\">Data Exploration & Description<a/></li>\n",
    "<li><a href=\"#preprocess\">Data Preprocessing<a/></li>\n",
    "<li><a href=\"#model\">Logistic Regression Model Training<a/></li>\n",
    "<li><a href=\"#cross\">Model Cross Validation<a/></li>\n",
    "<li><a href=\"#test\">Model Testing<a/></li>\n",
    "<li><a href=\"#conclusion\">Conclusion<a/></li>\n",
    "<li><a href=\"#reference\">References<a/></li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bd5afe",
   "metadata": {},
   "source": [
    "<a id=\"lib\"></a>\n",
    "### Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0691a860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Logistic regression model training and evaluation\n",
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Data visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abca8d1",
   "metadata": {},
   "source": [
    "<a id=\"aim\"></a>\n",
    "### Aim & Objectives\n",
    "\n",
    "> The primary aim of this work is to train a logistic regression model capable of predicting the occurrence of microearthquakes if provided with informative features which measure some underlying information. <br><br>\n",
    "To achieve this aim, the following specific objectives have been outlined: <br>\n",
    "1. Conduct adequate data exploration and preprocessing to ensure optimum model performance. <br>\n",
    "2. Conduct adequate training of a logistic regression model to predict microearthquakes to an acceptable degree of accuracy. <br>\n",
    "3. Validate and test the trained model with novel data points to ensure its ability to predict microearthquakes when provided with novel data. <br>\n",
    "4. Provide insight on modifications required to implement a tanh function in place of the logistic (or sigmoid) function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae7d729",
   "metadata": {},
   "source": [
    "<a id=\"explore\"></a>\n",
    "### Data Exploration & Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897508e3",
   "metadata": {},
   "source": [
    "> Brief Description of Dataset\n",
    "- Dataset contains microearthquake waveforms with labels indicating the occurrence or absence of microearthquakes\n",
    "- It hosts a total of 6800 data points and 100 features, indexed from 0 through 99.\n",
    "- Each feature is believed to measure and provide some underlying information about the occurrence or absence of microearthquakes\n",
    "- Labels contained include 1 or 0, where 1 corresponds to microearthquake and 0 corresponds to noise (absence of microearthquake)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70e1b4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the dataset and labels\n",
    "features, labels = np.load(\"features.npy\"), np.load(\"label.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da87558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the loaded dataset and labels into pandas dataframe for easy visualisation and statistical analysis\n",
    "df_features = pd.DataFrame(features)\n",
    "df_labels = pd.DataFrame(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7962d16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030990</td>\n",
       "      <td>0.005665</td>\n",
       "      <td>0.042319</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.233922</td>\n",
       "      <td>0.153615</td>\n",
       "      <td>0.648451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060251</td>\n",
       "      <td>0.060090</td>\n",
       "      <td>0.059935</td>\n",
       "      <td>0.059787</td>\n",
       "      <td>0.059644</td>\n",
       "      <td>0.059507</td>\n",
       "      <td>65.070959</td>\n",
       "      <td>0.021690</td>\n",
       "      <td>0.074659</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023992</td>\n",
       "      <td>0.001666</td>\n",
       "      <td>0.045985</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300900</td>\n",
       "      <td>0.193602</td>\n",
       "      <td>0.414528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009044</td>\n",
       "      <td>0.008956</td>\n",
       "      <td>0.008872</td>\n",
       "      <td>0.008790</td>\n",
       "      <td>0.008711</td>\n",
       "      <td>0.008635</td>\n",
       "      <td>74.326553</td>\n",
       "      <td>0.024776</td>\n",
       "      <td>0.073504</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.621460</td>\n",
       "      <td>0.305232</td>\n",
       "      <td>0.098634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.662831</td>\n",
       "      <td>0.661856</td>\n",
       "      <td>0.660910</td>\n",
       "      <td>0.660008</td>\n",
       "      <td>0.659142</td>\n",
       "      <td>0.658311</td>\n",
       "      <td>338.960143</td>\n",
       "      <td>0.112987</td>\n",
       "      <td>0.171816</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.627124</td>\n",
       "      <td>0.326891</td>\n",
       "      <td>0.092636</td>\n",
       "      <td>...</td>\n",
       "      <td>1.235368</td>\n",
       "      <td>1.232399</td>\n",
       "      <td>1.229523</td>\n",
       "      <td>1.226726</td>\n",
       "      <td>1.224060</td>\n",
       "      <td>1.221514</td>\n",
       "      <td>287.007675</td>\n",
       "      <td>0.095669</td>\n",
       "      <td>0.221686</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.620127</td>\n",
       "      <td>0.324892</td>\n",
       "      <td>0.106631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.695257</td>\n",
       "      <td>0.694793</td>\n",
       "      <td>0.694345</td>\n",
       "      <td>0.693912</td>\n",
       "      <td>0.693496</td>\n",
       "      <td>0.693095</td>\n",
       "      <td>690.902445</td>\n",
       "      <td>0.230301</td>\n",
       "      <td>0.303567</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1         2         3         4         5         6         7   \\\n",
       "0  0.0  0.0  0.030990  0.005665  0.042319  0.002333  0.000333  0.233922   \n",
       "1  0.0  0.0  0.023992  0.001666  0.045985  0.000000  0.000000  0.300900   \n",
       "2  1.0  0.0  0.005332  0.000000  0.015995  0.000000  0.000000  0.621460   \n",
       "3  1.0  0.0  0.002333  0.000000  0.010663  0.000000  0.000000  0.627124   \n",
       "4  1.0  1.0  0.001333  0.000000  0.008664  0.000000  0.000000  0.620127   \n",
       "\n",
       "         8         9   ...        90        91        92        93        94  \\\n",
       "0  0.153615  0.648451  ...  0.060251  0.060090  0.059935  0.059787  0.059644   \n",
       "1  0.193602  0.414528  ...  0.009044  0.008956  0.008872  0.008790  0.008711   \n",
       "2  0.305232  0.098634  ...  0.662831  0.661856  0.660910  0.660008  0.659142   \n",
       "3  0.326891  0.092636  ...  1.235368  1.232399  1.229523  1.226726  1.224060   \n",
       "4  0.324892  0.106631  ...  0.695257  0.694793  0.694345  0.693912  0.693496   \n",
       "\n",
       "         95          96        97        98        99  \n",
       "0  0.059507   65.070959  0.021690  0.074659  0.000002  \n",
       "1  0.008635   74.326553  0.024776  0.073504  0.000002  \n",
       "2  0.658311  338.960143  0.112987  0.171816  0.000004  \n",
       "3  1.221514  287.007675  0.095669  0.221686  0.000005  \n",
       "4  0.693095  690.902445  0.230301  0.303567  0.000006  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displays the first five rows of the features\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3035389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6800.000000</td>\n",
       "      <td>6800.000000</td>\n",
       "      <td>6800.000000</td>\n",
       "      <td>6800.000000</td>\n",
       "      <td>6800.000000</td>\n",
       "      <td>6800.000000</td>\n",
       "      <td>6800.000000</td>\n",
       "      <td>6800.000000</td>\n",
       "      <td>6800.000000</td>\n",
       "      <td>6800.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6800.000000</td>\n",
       "      <td>6800.000000</td>\n",
       "      <td>6800.000000</td>\n",
       "      <td>6800.000000</td>\n",
       "      <td>6800.000000</td>\n",
       "      <td>6800.000000</td>\n",
       "      <td>6800.000000</td>\n",
       "      <td>6800.000000</td>\n",
       "      <td>6800.000000</td>\n",
       "      <td>6.800000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.630735</td>\n",
       "      <td>0.133824</td>\n",
       "      <td>0.010993</td>\n",
       "      <td>0.001921</td>\n",
       "      <td>0.021678</td>\n",
       "      <td>0.000947</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.499814</td>\n",
       "      <td>0.255807</td>\n",
       "      <td>0.166239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.317939</td>\n",
       "      <td>0.316996</td>\n",
       "      <td>0.316082</td>\n",
       "      <td>0.315196</td>\n",
       "      <td>0.314340</td>\n",
       "      <td>0.313513</td>\n",
       "      <td>280.799204</td>\n",
       "      <td>0.093600</td>\n",
       "      <td>0.167322</td>\n",
       "      <td>3.526858e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.482641</td>\n",
       "      <td>0.340488</td>\n",
       "      <td>0.010682</td>\n",
       "      <td>0.003051</td>\n",
       "      <td>0.012459</td>\n",
       "      <td>0.001735</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.169834</td>\n",
       "      <td>0.084536</td>\n",
       "      <td>0.122678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.361851</td>\n",
       "      <td>0.361323</td>\n",
       "      <td>0.360822</td>\n",
       "      <td>0.360344</td>\n",
       "      <td>0.359892</td>\n",
       "      <td>0.359464</td>\n",
       "      <td>175.720220</td>\n",
       "      <td>0.058573</td>\n",
       "      <td>0.080605</td>\n",
       "      <td>1.699019e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.084305</td>\n",
       "      <td>0.044652</td>\n",
       "      <td>0.021993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>11.108712</td>\n",
       "      <td>0.003703</td>\n",
       "      <td>0.008730</td>\n",
       "      <td>1.840131e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.328890</td>\n",
       "      <td>0.172276</td>\n",
       "      <td>0.097967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044417</td>\n",
       "      <td>0.043959</td>\n",
       "      <td>0.043585</td>\n",
       "      <td>0.043120</td>\n",
       "      <td>0.042806</td>\n",
       "      <td>0.042413</td>\n",
       "      <td>104.638832</td>\n",
       "      <td>0.034880</td>\n",
       "      <td>0.089069</td>\n",
       "      <td>1.877437e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004665</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015661</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.604465</td>\n",
       "      <td>0.305898</td>\n",
       "      <td>0.108964</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189603</td>\n",
       "      <td>0.187924</td>\n",
       "      <td>0.186792</td>\n",
       "      <td>0.185971</td>\n",
       "      <td>0.184688</td>\n",
       "      <td>0.183253</td>\n",
       "      <td>304.787948</td>\n",
       "      <td>0.101596</td>\n",
       "      <td>0.180970</td>\n",
       "      <td>3.814556e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022326</td>\n",
       "      <td>0.003332</td>\n",
       "      <td>0.033989</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.617794</td>\n",
       "      <td>0.317561</td>\n",
       "      <td>0.180940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.475038</td>\n",
       "      <td>0.473890</td>\n",
       "      <td>0.473257</td>\n",
       "      <td>0.472597</td>\n",
       "      <td>0.471843</td>\n",
       "      <td>0.470792</td>\n",
       "      <td>404.396980</td>\n",
       "      <td>0.134799</td>\n",
       "      <td>0.233520</td>\n",
       "      <td>4.922196e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.042319</td>\n",
       "      <td>0.013329</td>\n",
       "      <td>0.052982</td>\n",
       "      <td>0.008664</td>\n",
       "      <td>0.005998</td>\n",
       "      <td>0.765412</td>\n",
       "      <td>0.482839</td>\n",
       "      <td>0.685438</td>\n",
       "      <td>...</td>\n",
       "      <td>3.086388</td>\n",
       "      <td>3.080982</td>\n",
       "      <td>3.075712</td>\n",
       "      <td>3.070699</td>\n",
       "      <td>3.065876</td>\n",
       "      <td>3.061250</td>\n",
       "      <td>1431.750187</td>\n",
       "      <td>0.477250</td>\n",
       "      <td>0.483015</td>\n",
       "      <td>1.018116e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3            4   \\\n",
       "count  6800.000000  6800.000000  6800.000000  6800.000000  6800.000000   \n",
       "mean      0.630735     0.133824     0.010993     0.001921     0.021678   \n",
       "std       0.482641     0.340488     0.010682     0.003051     0.012459   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.002666     0.000000     0.012329   \n",
       "50%       1.000000     0.000000     0.004665     0.000000     0.015661   \n",
       "75%       1.000000     0.000000     0.022326     0.003332     0.033989   \n",
       "max       1.000000     1.000000     0.042319     0.013329     0.052982   \n",
       "\n",
       "                5            6            7            8            9   ...  \\\n",
       "count  6800.000000  6800.000000  6800.000000  6800.000000  6800.000000  ...   \n",
       "mean      0.000947     0.000485     0.499814     0.255807     0.166239  ...   \n",
       "std       0.001735     0.001044     0.169834     0.084536     0.122678  ...   \n",
       "min       0.000000     0.000000     0.084305     0.044652     0.021993  ...   \n",
       "25%       0.000000     0.000000     0.328890     0.172276     0.097967  ...   \n",
       "50%       0.000000     0.000000     0.604465     0.305898     0.108964  ...   \n",
       "75%       0.001000     0.000333     0.617794     0.317561     0.180940  ...   \n",
       "max       0.008664     0.005998     0.765412     0.482839     0.685438  ...   \n",
       "\n",
       "                90           91           92           93           94  \\\n",
       "count  6800.000000  6800.000000  6800.000000  6800.000000  6800.000000   \n",
       "mean      0.317939     0.316996     0.316082     0.315196     0.314340   \n",
       "std       0.361851     0.361323     0.360822     0.360344     0.359892   \n",
       "min       0.000109     0.000111     0.000079     0.000076     0.000117   \n",
       "25%       0.044417     0.043959     0.043585     0.043120     0.042806   \n",
       "50%       0.189603     0.187924     0.186792     0.185971     0.184688   \n",
       "75%       0.475038     0.473890     0.473257     0.472597     0.471843   \n",
       "max       3.086388     3.080982     3.075712     3.070699     3.065876   \n",
       "\n",
       "                95           96           97           98            99  \n",
       "count  6800.000000  6800.000000  6800.000000  6800.000000  6.800000e+03  \n",
       "mean      0.313513   280.799204     0.093600     0.167322  3.526858e-06  \n",
       "std       0.359464   175.720220     0.058573     0.080605  1.699019e-06  \n",
       "min       0.000118    11.108712     0.003703     0.008730  1.840131e-07  \n",
       "25%       0.042413   104.638832     0.034880     0.089069  1.877437e-06  \n",
       "50%       0.183253   304.787948     0.101596     0.180970  3.814556e-06  \n",
       "75%       0.470792   404.396980     0.134799     0.233520  4.922196e-06  \n",
       "max       3.061250  1431.750187     0.477250     0.483015  1.018116e-05  \n",
       "\n",
       "[8 rows x 100 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# provides summary statistics of the features\n",
    "df_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74fa0d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "0  1.0\n",
       "1  1.0\n",
       "2  0.0\n",
       "3  0.0\n",
       "4  0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displays the first five rows of the labels\n",
    "df_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "316677d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    4648\n",
       "1.0    2152\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displays the number of unique values within the labels\n",
    "df_labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3c1b0d",
   "metadata": {},
   "source": [
    "> From the summary statistics of the features presented, it can be observed that the features exist on largely varying scales. The feature indexed at 0 has a range of 1, which is in sharp contrast to the feature indexed at 96, which has a range of 1,420. A similar deduction can be observed for other features as well.<br><br>\n",
    "For machine learning models such as logistic regression which employ the use of gradient descent, the utilization of features that exist on largely different scales may hinder the ability of gradient descent to converge swiftly (Gasso, 2019; Alshaher, 2021). The integration of feature scaling in the preprocessing stage is a solution to this challenge (Bisong, and Bisong, 2019). Therefore, given the variation in the scale of the features to be utilized for this work, feature scaling was integrated into the data preprocessing stage.<br><br>\n",
    "The Scikit-Learn python library (Pedregosa, et al., 2011) utilized for the course of this work offers two methods for feature scaling; the first being normalization (MinMaxScaler) and the second being standardization (StandardScaler). While the difference between both methods is not pronounced, the standardization method (StandardScaler) was employed in carrying out feature scaling for this work because, unlike the normalization method, it preserves information about potential outliers within the dataset.<br><br>\n",
    "From the .value_count() function called on the labels, it was confirmed that the ML problem is indeed a binary classification problem that can be handled by a logistic regression model. From this, it was also deduced that majority of our data points (4648 or ~68%) are noise and only a small fraction (2152 or ~30%) represent microearthquakes. This information buttressed the importance of ensuring that the data points were randomly distributed across the training, validation, and testing datasets to utilized.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c54045",
   "metadata": {},
   "source": [
    "<a id=\"preprocess\"></a>\n",
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f322d5",
   "metadata": {},
   "source": [
    "> Information obtained from the data exploration and description stage informed the preprocessing stage and determined the type of preprocessing carried out on the dataset. Here, the primary preprocessing method integrated into this workflow was feature scaling. This process was subsequently followed by dataset splitting. <br><br>\n",
    "Although the two-way data splitting scheme (training and testing) is not uncommon, for this work, the three-way data splitting scheme (Training, Cros validation, and Testing) was employed. While the cross-validation and testing phase are in essence a repetition, albeit with novel data points, the cross-validation phase was included because a greater amount of confidence can be ascribed to the performance of the model if it performs adequately on two sets of novel data points (cross-validation and test) as against using just the testing datase3t (Reitermanova, 2010).<br><br>\n",
    "By convention, the 60-20-20 (approximately) convention corresponding to the size of the training, cross-validation, and testing data points was utilized, as this convention ensured that a large proportion of the data points were used in training the model.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fa7909",
   "metadata": {},
   "source": [
    "##### Step 1: Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc151584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scales the features\n",
    "scaled_df_features = StandardScaler().fit_transform(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7562eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.800000e+03</td>\n",
       "      <td>6.800000e+03</td>\n",
       "      <td>6.800000e+03</td>\n",
       "      <td>6.800000e+03</td>\n",
       "      <td>6.800000e+03</td>\n",
       "      <td>6.800000e+03</td>\n",
       "      <td>6.800000e+03</td>\n",
       "      <td>6.800000e+03</td>\n",
       "      <td>6.800000e+03</td>\n",
       "      <td>6.800000e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>6.800000e+03</td>\n",
       "      <td>6.800000e+03</td>\n",
       "      <td>6.800000e+03</td>\n",
       "      <td>6.800000e+03</td>\n",
       "      <td>6.800000e+03</td>\n",
       "      <td>6.800000e+03</td>\n",
       "      <td>6.800000e+03</td>\n",
       "      <td>6.800000e+03</td>\n",
       "      <td>6.800000e+03</td>\n",
       "      <td>6.800000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-2.841191e-16</td>\n",
       "      <td>-9.465468e-17</td>\n",
       "      <td>2.160200e-16</td>\n",
       "      <td>4.754367e-17</td>\n",
       "      <td>-3.562510e-16</td>\n",
       "      <td>-3.830269e-16</td>\n",
       "      <td>4.441219e-16</td>\n",
       "      <td>-1.937013e-16</td>\n",
       "      <td>-1.654396e-16</td>\n",
       "      <td>1.000017e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.080069e-16</td>\n",
       "      <td>-7.244920e-17</td>\n",
       "      <td>1.053518e-16</td>\n",
       "      <td>-1.315094e-16</td>\n",
       "      <td>-1.905237e-17</td>\n",
       "      <td>1.942921e-16</td>\n",
       "      <td>-2.327876e-16</td>\n",
       "      <td>1.417167e-17</td>\n",
       "      <td>-9.350364e-17</td>\n",
       "      <td>3.003480e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000074e+00</td>\n",
       "      <td>1.000074e+00</td>\n",
       "      <td>1.000074e+00</td>\n",
       "      <td>1.000074e+00</td>\n",
       "      <td>1.000074e+00</td>\n",
       "      <td>1.000074e+00</td>\n",
       "      <td>1.000074e+00</td>\n",
       "      <td>1.000074e+00</td>\n",
       "      <td>1.000074e+00</td>\n",
       "      <td>1.000074e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000074e+00</td>\n",
       "      <td>1.000074e+00</td>\n",
       "      <td>1.000074e+00</td>\n",
       "      <td>1.000074e+00</td>\n",
       "      <td>1.000074e+00</td>\n",
       "      <td>1.000074e+00</td>\n",
       "      <td>1.000074e+00</td>\n",
       "      <td>1.000074e+00</td>\n",
       "      <td>1.000074e+00</td>\n",
       "      <td>1.000074e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.306937e+00</td>\n",
       "      <td>-3.930638e-01</td>\n",
       "      <td>-1.029144e+00</td>\n",
       "      <td>-6.295263e-01</td>\n",
       "      <td>-1.740117e+00</td>\n",
       "      <td>-5.457712e-01</td>\n",
       "      <td>-4.647685e-01</td>\n",
       "      <td>-2.446742e+00</td>\n",
       "      <td>-2.498009e+00</td>\n",
       "      <td>-1.175901e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.784112e-01</td>\n",
       "      <td>-8.770770e-01</td>\n",
       "      <td>-8.758513e-01</td>\n",
       "      <td>-8.745629e-01</td>\n",
       "      <td>-8.731696e-01</td>\n",
       "      <td>-8.719045e-01</td>\n",
       "      <td>-1.534885e+00</td>\n",
       "      <td>-1.534885e+00</td>\n",
       "      <td>-1.967659e+00</td>\n",
       "      <td>-1.967659e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.306937e+00</td>\n",
       "      <td>-3.930638e-01</td>\n",
       "      <td>-7.795743e-01</td>\n",
       "      <td>-6.295263e-01</td>\n",
       "      <td>-7.504464e-01</td>\n",
       "      <td>-5.457712e-01</td>\n",
       "      <td>-4.647685e-01</td>\n",
       "      <td>-1.006492e+00</td>\n",
       "      <td>-9.881880e-01</td>\n",
       "      <td>-5.565518e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.559545e-01</td>\n",
       "      <td>-7.557139e-01</td>\n",
       "      <td>-7.552682e-01</td>\n",
       "      <td>-7.550998e-01</td>\n",
       "      <td>-7.545428e-01</td>\n",
       "      <td>-7.542320e-01</td>\n",
       "      <td>-1.002579e+00</td>\n",
       "      <td>-1.002579e+00</td>\n",
       "      <td>-9.708799e-01</td>\n",
       "      <td>-9.708792e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.651478e-01</td>\n",
       "      <td>-3.930638e-01</td>\n",
       "      <td>-5.923972e-01</td>\n",
       "      <td>-6.295263e-01</td>\n",
       "      <td>-4.829679e-01</td>\n",
       "      <td>-5.457712e-01</td>\n",
       "      <td>-4.647685e-01</td>\n",
       "      <td>6.162434e-01</td>\n",
       "      <td>5.925909e-01</td>\n",
       "      <td>-4.669092e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.546921e-01</td>\n",
       "      <td>-3.572473e-01</td>\n",
       "      <td>-3.583481e-01</td>\n",
       "      <td>-3.586415e-01</td>\n",
       "      <td>-3.602792e-01</td>\n",
       "      <td>-3.623999e-01</td>\n",
       "      <td>1.365267e-01</td>\n",
       "      <td>1.365267e-01</td>\n",
       "      <td>1.693421e-01</td>\n",
       "      <td>1.693439e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.651478e-01</td>\n",
       "      <td>-3.930638e-01</td>\n",
       "      <td>1.061001e+00</td>\n",
       "      <td>4.625649e-01</td>\n",
       "      <td>9.881640e-01</td>\n",
       "      <td>3.050895e-02</td>\n",
       "      <td>-1.454368e-01</td>\n",
       "      <td>6.947311e-01</td>\n",
       "      <td>7.305641e-01</td>\n",
       "      <td>1.198422e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>4.341837e-01</td>\n",
       "      <td>4.342526e-01</td>\n",
       "      <td>4.356347e-01</td>\n",
       "      <td>4.368395e-01</td>\n",
       "      <td>4.376705e-01</td>\n",
       "      <td>4.375700e-01</td>\n",
       "      <td>7.034299e-01</td>\n",
       "      <td>7.034299e-01</td>\n",
       "      <td>8.213247e-01</td>\n",
       "      <td>8.213211e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.651478e-01</td>\n",
       "      <td>2.544116e+00</td>\n",
       "      <td>2.932772e+00</td>\n",
       "      <td>3.738838e+00</td>\n",
       "      <td>2.512792e+00</td>\n",
       "      <td>4.448657e+00</td>\n",
       "      <td>5.283202e+00</td>\n",
       "      <td>1.563983e+00</td>\n",
       "      <td>2.685842e+00</td>\n",
       "      <td>4.232535e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.651360e+00</td>\n",
       "      <td>7.650180e+00</td>\n",
       "      <td>7.648740e+00</td>\n",
       "      <td>7.647422e+00</td>\n",
       "      <td>7.646009e+00</td>\n",
       "      <td>7.644546e+00</td>\n",
       "      <td>6.550388e+00</td>\n",
       "      <td>6.550388e+00</td>\n",
       "      <td>3.916836e+00</td>\n",
       "      <td>3.916841e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0             1             2             3             4   \\\n",
       "count  6.800000e+03  6.800000e+03  6.800000e+03  6.800000e+03  6.800000e+03   \n",
       "mean  -2.841191e-16 -9.465468e-17  2.160200e-16  4.754367e-17 -3.562510e-16   \n",
       "std    1.000074e+00  1.000074e+00  1.000074e+00  1.000074e+00  1.000074e+00   \n",
       "min   -1.306937e+00 -3.930638e-01 -1.029144e+00 -6.295263e-01 -1.740117e+00   \n",
       "25%   -1.306937e+00 -3.930638e-01 -7.795743e-01 -6.295263e-01 -7.504464e-01   \n",
       "50%    7.651478e-01 -3.930638e-01 -5.923972e-01 -6.295263e-01 -4.829679e-01   \n",
       "75%    7.651478e-01 -3.930638e-01  1.061001e+00  4.625649e-01  9.881640e-01   \n",
       "max    7.651478e-01  2.544116e+00  2.932772e+00  3.738838e+00  2.512792e+00   \n",
       "\n",
       "                 5             6             7             8             9   \\\n",
       "count  6.800000e+03  6.800000e+03  6.800000e+03  6.800000e+03  6.800000e+03   \n",
       "mean  -3.830269e-16  4.441219e-16 -1.937013e-16 -1.654396e-16  1.000017e-16   \n",
       "std    1.000074e+00  1.000074e+00  1.000074e+00  1.000074e+00  1.000074e+00   \n",
       "min   -5.457712e-01 -4.647685e-01 -2.446742e+00 -2.498009e+00 -1.175901e+00   \n",
       "25%   -5.457712e-01 -4.647685e-01 -1.006492e+00 -9.881880e-01 -5.565518e-01   \n",
       "50%   -5.457712e-01 -4.647685e-01  6.162434e-01  5.925909e-01 -4.669092e-01   \n",
       "75%    3.050895e-02 -1.454368e-01  6.947311e-01  7.305641e-01  1.198422e-01   \n",
       "max    4.448657e+00  5.283202e+00  1.563983e+00  2.685842e+00  4.232535e+00   \n",
       "\n",
       "       ...            90            91            92            93  \\\n",
       "count  ...  6.800000e+03  6.800000e+03  6.800000e+03  6.800000e+03   \n",
       "mean   ... -1.080069e-16 -7.244920e-17  1.053518e-16 -1.315094e-16   \n",
       "std    ...  1.000074e+00  1.000074e+00  1.000074e+00  1.000074e+00   \n",
       "min    ... -8.784112e-01 -8.770770e-01 -8.758513e-01 -8.745629e-01   \n",
       "25%    ... -7.559545e-01 -7.557139e-01 -7.552682e-01 -7.550998e-01   \n",
       "50%    ... -3.546921e-01 -3.572473e-01 -3.583481e-01 -3.586415e-01   \n",
       "75%    ...  4.341837e-01  4.342526e-01  4.356347e-01  4.368395e-01   \n",
       "max    ...  7.651360e+00  7.650180e+00  7.648740e+00  7.647422e+00   \n",
       "\n",
       "                 94            95            96            97            98  \\\n",
       "count  6.800000e+03  6.800000e+03  6.800000e+03  6.800000e+03  6.800000e+03   \n",
       "mean  -1.905237e-17  1.942921e-16 -2.327876e-16  1.417167e-17 -9.350364e-17   \n",
       "std    1.000074e+00  1.000074e+00  1.000074e+00  1.000074e+00  1.000074e+00   \n",
       "min   -8.731696e-01 -8.719045e-01 -1.534885e+00 -1.534885e+00 -1.967659e+00   \n",
       "25%   -7.545428e-01 -7.542320e-01 -1.002579e+00 -1.002579e+00 -9.708799e-01   \n",
       "50%   -3.602792e-01 -3.623999e-01  1.365267e-01  1.365267e-01  1.693421e-01   \n",
       "75%    4.376705e-01  4.375700e-01  7.034299e-01  7.034299e-01  8.213247e-01   \n",
       "max    7.646009e+00  7.644546e+00  6.550388e+00  6.550388e+00  3.916836e+00   \n",
       "\n",
       "                 99  \n",
       "count  6.800000e+03  \n",
       "mean   3.003480e-16  \n",
       "std    1.000074e+00  \n",
       "min   -1.967659e+00  \n",
       "25%   -9.708792e-01  \n",
       "50%    1.693439e-01  \n",
       "75%    8.213211e-01  \n",
       "max    3.916841e+00  \n",
       "\n",
       "[8 rows x 100 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displays the summary statistics of scaled features\n",
    "pd.DataFrame(scaled_df_features).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5089b74",
   "metadata": {},
   "source": [
    "> From the result of the summary statistics displayed above, It can be observed that the range for all features roughly varies between -3 through 3. Thus, indicating that the features have been successfully scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511a6182",
   "metadata": {},
   "source": [
    "##### Step 2: Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8718b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the dataset into test and main dataset after shuffling\n",
    "main_set, test_set, main_labels, test_labels = train_test_split(scaled_df_features, np.array(df_labels), test_size= 1300, random_state=5)\n",
    "\n",
    "# Splits the main dataset into train and cross validation datasets after re-shuffling main_set and main_labels\n",
    "train_set, cross_val_set, train_labels, cross_val_labels = train_test_split(main_set, main_labels, test_size= 1300, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a8ccadf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4200.000000</td>\n",
       "      <td>4200.000000</td>\n",
       "      <td>4200.000000</td>\n",
       "      <td>4200.000000</td>\n",
       "      <td>4200.000000</td>\n",
       "      <td>4200.000000</td>\n",
       "      <td>4200.000000</td>\n",
       "      <td>4200.000000</td>\n",
       "      <td>4200.000000</td>\n",
       "      <td>4200.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4200.000000</td>\n",
       "      <td>4200.000000</td>\n",
       "      <td>4200.000000</td>\n",
       "      <td>4200.000000</td>\n",
       "      <td>4200.000000</td>\n",
       "      <td>4200.000000</td>\n",
       "      <td>4200.000000</td>\n",
       "      <td>4200.000000</td>\n",
       "      <td>4200.000000</td>\n",
       "      <td>4200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.015337</td>\n",
       "      <td>-0.019622</td>\n",
       "      <td>0.009912</td>\n",
       "      <td>0.010309</td>\n",
       "      <td>0.010052</td>\n",
       "      <td>0.008830</td>\n",
       "      <td>0.008603</td>\n",
       "      <td>-0.011906</td>\n",
       "      <td>-0.012973</td>\n",
       "      <td>0.008255</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018135</td>\n",
       "      <td>-0.018137</td>\n",
       "      <td>-0.018138</td>\n",
       "      <td>-0.018137</td>\n",
       "      <td>-0.018136</td>\n",
       "      <td>-0.018134</td>\n",
       "      <td>-0.014449</td>\n",
       "      <td>-0.014449</td>\n",
       "      <td>-0.010857</td>\n",
       "      <td>-0.010857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.004149</td>\n",
       "      <td>0.978588</td>\n",
       "      <td>1.001216</td>\n",
       "      <td>1.001927</td>\n",
       "      <td>1.001227</td>\n",
       "      <td>1.001168</td>\n",
       "      <td>1.004519</td>\n",
       "      <td>1.004681</td>\n",
       "      <td>1.004520</td>\n",
       "      <td>1.001652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.971785</td>\n",
       "      <td>0.971737</td>\n",
       "      <td>0.971693</td>\n",
       "      <td>0.971651</td>\n",
       "      <td>0.971610</td>\n",
       "      <td>0.971571</td>\n",
       "      <td>0.990396</td>\n",
       "      <td>0.990396</td>\n",
       "      <td>0.998476</td>\n",
       "      <td>0.998476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.306937</td>\n",
       "      <td>-0.393064</td>\n",
       "      <td>-1.029144</td>\n",
       "      <td>-0.629526</td>\n",
       "      <td>-1.740117</td>\n",
       "      <td>-0.545771</td>\n",
       "      <td>-0.464768</td>\n",
       "      <td>-2.446742</td>\n",
       "      <td>-2.474357</td>\n",
       "      <td>-1.175901</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.878382</td>\n",
       "      <td>-0.877054</td>\n",
       "      <td>-0.875741</td>\n",
       "      <td>-0.874445</td>\n",
       "      <td>-0.873166</td>\n",
       "      <td>-0.871905</td>\n",
       "      <td>-1.534885</td>\n",
       "      <td>-1.534885</td>\n",
       "      <td>-1.967659</td>\n",
       "      <td>-1.967659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.306937</td>\n",
       "      <td>-0.393064</td>\n",
       "      <td>-0.779574</td>\n",
       "      <td>-0.629526</td>\n",
       "      <td>-0.750446</td>\n",
       "      <td>-0.545771</td>\n",
       "      <td>-0.464768</td>\n",
       "      <td>-1.046226</td>\n",
       "      <td>-1.019725</td>\n",
       "      <td>-0.553835</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.761835</td>\n",
       "      <td>-0.761624</td>\n",
       "      <td>-0.761157</td>\n",
       "      <td>-0.760372</td>\n",
       "      <td>-0.759817</td>\n",
       "      <td>-0.759515</td>\n",
       "      <td>-1.010630</td>\n",
       "      <td>-1.010630</td>\n",
       "      <td>-0.977775</td>\n",
       "      <td>-0.977774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.765148</td>\n",
       "      <td>-0.393064</td>\n",
       "      <td>-0.592397</td>\n",
       "      <td>-0.629526</td>\n",
       "      <td>-0.482968</td>\n",
       "      <td>-0.545771</td>\n",
       "      <td>-0.464768</td>\n",
       "      <td>0.614281</td>\n",
       "      <td>0.584707</td>\n",
       "      <td>-0.461476</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.357970</td>\n",
       "      <td>-0.359298</td>\n",
       "      <td>-0.360747</td>\n",
       "      <td>-0.362318</td>\n",
       "      <td>-0.363005</td>\n",
       "      <td>-0.364723</td>\n",
       "      <td>0.114768</td>\n",
       "      <td>0.114768</td>\n",
       "      <td>0.144770</td>\n",
       "      <td>0.144763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.765148</td>\n",
       "      <td>-0.393064</td>\n",
       "      <td>1.061001</td>\n",
       "      <td>0.571774</td>\n",
       "      <td>1.014912</td>\n",
       "      <td>0.222602</td>\n",
       "      <td>-0.145437</td>\n",
       "      <td>0.694731</td>\n",
       "      <td>0.730564</td>\n",
       "      <td>0.138857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.433745</td>\n",
       "      <td>0.433840</td>\n",
       "      <td>0.433329</td>\n",
       "      <td>0.432477</td>\n",
       "      <td>0.433384</td>\n",
       "      <td>0.433785</td>\n",
       "      <td>0.700903</td>\n",
       "      <td>0.700903</td>\n",
       "      <td>0.810785</td>\n",
       "      <td>0.810788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.765148</td>\n",
       "      <td>2.544116</td>\n",
       "      <td>2.932772</td>\n",
       "      <td>3.738838</td>\n",
       "      <td>2.512792</td>\n",
       "      <td>4.064470</td>\n",
       "      <td>5.283202</td>\n",
       "      <td>1.563983</td>\n",
       "      <td>2.685842</td>\n",
       "      <td>4.107579</td>\n",
       "      <td>...</td>\n",
       "      <td>7.046945</td>\n",
       "      <td>7.046829</td>\n",
       "      <td>7.046622</td>\n",
       "      <td>7.046337</td>\n",
       "      <td>7.045962</td>\n",
       "      <td>7.045499</td>\n",
       "      <td>6.539433</td>\n",
       "      <td>6.539433</td>\n",
       "      <td>3.916836</td>\n",
       "      <td>3.916841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3            4   \\\n",
       "count  4200.000000  4200.000000  4200.000000  4200.000000  4200.000000   \n",
       "mean     -0.015337    -0.019622     0.009912     0.010309     0.010052   \n",
       "std       1.004149     0.978588     1.001216     1.001927     1.001227   \n",
       "min      -1.306937    -0.393064    -1.029144    -0.629526    -1.740117   \n",
       "25%      -1.306937    -0.393064    -0.779574    -0.629526    -0.750446   \n",
       "50%       0.765148    -0.393064    -0.592397    -0.629526    -0.482968   \n",
       "75%       0.765148    -0.393064     1.061001     0.571774     1.014912   \n",
       "max       0.765148     2.544116     2.932772     3.738838     2.512792   \n",
       "\n",
       "                5            6            7            8            9   ...  \\\n",
       "count  4200.000000  4200.000000  4200.000000  4200.000000  4200.000000  ...   \n",
       "mean      0.008830     0.008603    -0.011906    -0.012973     0.008255  ...   \n",
       "std       1.001168     1.004519     1.004681     1.004520     1.001652  ...   \n",
       "min      -0.545771    -0.464768    -2.446742    -2.474357    -1.175901  ...   \n",
       "25%      -0.545771    -0.464768    -1.046226    -1.019725    -0.553835  ...   \n",
       "50%      -0.545771    -0.464768     0.614281     0.584707    -0.461476  ...   \n",
       "75%       0.222602    -0.145437     0.694731     0.730564     0.138857  ...   \n",
       "max       4.064470     5.283202     1.563983     2.685842     4.107579  ...   \n",
       "\n",
       "                90           91           92           93           94  \\\n",
       "count  4200.000000  4200.000000  4200.000000  4200.000000  4200.000000   \n",
       "mean     -0.018135    -0.018137    -0.018138    -0.018137    -0.018136   \n",
       "std       0.971785     0.971737     0.971693     0.971651     0.971610   \n",
       "min      -0.878382    -0.877054    -0.875741    -0.874445    -0.873166   \n",
       "25%      -0.761835    -0.761624    -0.761157    -0.760372    -0.759817   \n",
       "50%      -0.357970    -0.359298    -0.360747    -0.362318    -0.363005   \n",
       "75%       0.433745     0.433840     0.433329     0.432477     0.433384   \n",
       "max       7.046945     7.046829     7.046622     7.046337     7.045962   \n",
       "\n",
       "                95           96           97           98           99  \n",
       "count  4200.000000  4200.000000  4200.000000  4200.000000  4200.000000  \n",
       "mean     -0.018134    -0.014449    -0.014449    -0.010857    -0.010857  \n",
       "std       0.971571     0.990396     0.990396     0.998476     0.998476  \n",
       "min      -0.871905    -1.534885    -1.534885    -1.967659    -1.967659  \n",
       "25%      -0.759515    -1.010630    -1.010630    -0.977775    -0.977774  \n",
       "50%      -0.364723     0.114768     0.114768     0.144770     0.144763  \n",
       "75%       0.433785     0.700903     0.700903     0.810785     0.810788  \n",
       "max       7.045499     6.539433     6.539433     3.916836     3.916841  \n",
       "\n",
       "[8 rows x 100 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displays the summary statistics of scaled training features\n",
    "pd.DataFrame(train_set).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25048336",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.012822</td>\n",
       "      <td>0.058810</td>\n",
       "      <td>-0.009844</td>\n",
       "      <td>0.004139</td>\n",
       "      <td>-0.013811</td>\n",
       "      <td>0.003025</td>\n",
       "      <td>-0.007879</td>\n",
       "      <td>0.015425</td>\n",
       "      <td>0.009198</td>\n",
       "      <td>-0.039607</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009629</td>\n",
       "      <td>0.009549</td>\n",
       "      <td>0.009474</td>\n",
       "      <td>0.009398</td>\n",
       "      <td>0.009323</td>\n",
       "      <td>0.009248</td>\n",
       "      <td>0.020484</td>\n",
       "      <td>0.020484</td>\n",
       "      <td>0.005592</td>\n",
       "      <td>0.005592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.996822</td>\n",
       "      <td>1.060146</td>\n",
       "      <td>0.999846</td>\n",
       "      <td>1.020261</td>\n",
       "      <td>0.999269</td>\n",
       "      <td>1.012282</td>\n",
       "      <td>0.994414</td>\n",
       "      <td>0.990459</td>\n",
       "      <td>0.999442</td>\n",
       "      <td>0.953611</td>\n",
       "      <td>...</td>\n",
       "      <td>1.020651</td>\n",
       "      <td>1.020696</td>\n",
       "      <td>1.020738</td>\n",
       "      <td>1.020779</td>\n",
       "      <td>1.020820</td>\n",
       "      <td>1.020859</td>\n",
       "      <td>1.031627</td>\n",
       "      <td>1.031627</td>\n",
       "      <td>1.010380</td>\n",
       "      <td>1.010380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.306937</td>\n",
       "      <td>-0.393064</td>\n",
       "      <td>-1.029144</td>\n",
       "      <td>-0.629526</td>\n",
       "      <td>-1.740117</td>\n",
       "      <td>-0.545771</td>\n",
       "      <td>-0.464768</td>\n",
       "      <td>-2.385914</td>\n",
       "      <td>-2.304847</td>\n",
       "      <td>-1.040078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.878266</td>\n",
       "      <td>-0.876934</td>\n",
       "      <td>-0.875617</td>\n",
       "      <td>-0.874317</td>\n",
       "      <td>-0.873034</td>\n",
       "      <td>-0.871769</td>\n",
       "      <td>-1.517327</td>\n",
       "      <td>-1.517327</td>\n",
       "      <td>-1.903898</td>\n",
       "      <td>-1.903899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.306937</td>\n",
       "      <td>-0.393064</td>\n",
       "      <td>-0.779574</td>\n",
       "      <td>-0.629526</td>\n",
       "      <td>-0.750446</td>\n",
       "      <td>-0.545771</td>\n",
       "      <td>-0.464768</td>\n",
       "      <td>-0.965285</td>\n",
       "      <td>-0.954680</td>\n",
       "      <td>-0.559268</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.738100</td>\n",
       "      <td>-0.737439</td>\n",
       "      <td>-0.736489</td>\n",
       "      <td>-0.735669</td>\n",
       "      <td>-0.734912</td>\n",
       "      <td>-0.734239</td>\n",
       "      <td>-0.985860</td>\n",
       "      <td>-0.985860</td>\n",
       "      <td>-0.975250</td>\n",
       "      <td>-0.975249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.765148</td>\n",
       "      <td>-0.393064</td>\n",
       "      <td>-0.592397</td>\n",
       "      <td>-0.629526</td>\n",
       "      <td>-0.509716</td>\n",
       "      <td>-0.545771</td>\n",
       "      <td>-0.464768</td>\n",
       "      <td>0.617224</td>\n",
       "      <td>0.600475</td>\n",
       "      <td>-0.477775</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.359141</td>\n",
       "      <td>-0.362743</td>\n",
       "      <td>-0.366071</td>\n",
       "      <td>-0.370449</td>\n",
       "      <td>-0.371107</td>\n",
       "      <td>-0.370790</td>\n",
       "      <td>0.161859</td>\n",
       "      <td>0.161859</td>\n",
       "      <td>0.199105</td>\n",
       "      <td>0.199105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.765148</td>\n",
       "      <td>-0.393064</td>\n",
       "      <td>1.092197</td>\n",
       "      <td>0.462565</td>\n",
       "      <td>0.934668</td>\n",
       "      <td>0.030509</td>\n",
       "      <td>-0.464768</td>\n",
       "      <td>0.696693</td>\n",
       "      <td>0.734506</td>\n",
       "      <td>0.069588</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408479</td>\n",
       "      <td>0.409549</td>\n",
       "      <td>0.409251</td>\n",
       "      <td>0.410284</td>\n",
       "      <td>0.409061</td>\n",
       "      <td>0.406839</td>\n",
       "      <td>0.712596</td>\n",
       "      <td>0.712596</td>\n",
       "      <td>0.844414</td>\n",
       "      <td>0.844417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.765148</td>\n",
       "      <td>2.544116</td>\n",
       "      <td>2.277652</td>\n",
       "      <td>3.411211</td>\n",
       "      <td>2.298809</td>\n",
       "      <td>3.680284</td>\n",
       "      <td>5.283202</td>\n",
       "      <td>1.436441</td>\n",
       "      <td>1.988091</td>\n",
       "      <td>4.126594</td>\n",
       "      <td>...</td>\n",
       "      <td>7.651360</td>\n",
       "      <td>7.650180</td>\n",
       "      <td>7.648740</td>\n",
       "      <td>7.647422</td>\n",
       "      <td>7.646009</td>\n",
       "      <td>7.644546</td>\n",
       "      <td>6.539759</td>\n",
       "      <td>6.539759</td>\n",
       "      <td>3.235443</td>\n",
       "      <td>3.235442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3            4   \\\n",
       "count  1300.000000  1300.000000  1300.000000  1300.000000  1300.000000   \n",
       "mean      0.012822     0.058810    -0.009844     0.004139    -0.013811   \n",
       "std       0.996822     1.060146     0.999846     1.020261     0.999269   \n",
       "min      -1.306937    -0.393064    -1.029144    -0.629526    -1.740117   \n",
       "25%      -1.306937    -0.393064    -0.779574    -0.629526    -0.750446   \n",
       "50%       0.765148    -0.393064    -0.592397    -0.629526    -0.509716   \n",
       "75%       0.765148    -0.393064     1.092197     0.462565     0.934668   \n",
       "max       0.765148     2.544116     2.277652     3.411211     2.298809   \n",
       "\n",
       "                5            6            7            8            9   ...  \\\n",
       "count  1300.000000  1300.000000  1300.000000  1300.000000  1300.000000  ...   \n",
       "mean      0.003025    -0.007879     0.015425     0.009198    -0.039607  ...   \n",
       "std       1.012282     0.994414     0.990459     0.999442     0.953611  ...   \n",
       "min      -0.545771    -0.464768    -2.385914    -2.304847    -1.040078  ...   \n",
       "25%      -0.545771    -0.464768    -0.965285    -0.954680    -0.559268  ...   \n",
       "50%      -0.545771    -0.464768     0.617224     0.600475    -0.477775  ...   \n",
       "75%       0.030509    -0.464768     0.696693     0.734506     0.069588  ...   \n",
       "max       3.680284     5.283202     1.436441     1.988091     4.126594  ...   \n",
       "\n",
       "                90           91           92           93           94  \\\n",
       "count  1300.000000  1300.000000  1300.000000  1300.000000  1300.000000   \n",
       "mean      0.009629     0.009549     0.009474     0.009398     0.009323   \n",
       "std       1.020651     1.020696     1.020738     1.020779     1.020820   \n",
       "min      -0.878266    -0.876934    -0.875617    -0.874317    -0.873034   \n",
       "25%      -0.738100    -0.737439    -0.736489    -0.735669    -0.734912   \n",
       "50%      -0.359141    -0.362743    -0.366071    -0.370449    -0.371107   \n",
       "75%       0.408479     0.409549     0.409251     0.410284     0.409061   \n",
       "max       7.651360     7.650180     7.648740     7.647422     7.646009   \n",
       "\n",
       "                95           96           97           98           99  \n",
       "count  1300.000000  1300.000000  1300.000000  1300.000000  1300.000000  \n",
       "mean      0.009248     0.020484     0.020484     0.005592     0.005592  \n",
       "std       1.020859     1.031627     1.031627     1.010380     1.010380  \n",
       "min      -0.871769    -1.517327    -1.517327    -1.903898    -1.903899  \n",
       "25%      -0.734239    -0.985860    -0.985860    -0.975250    -0.975249  \n",
       "50%      -0.370790     0.161859     0.161859     0.199105     0.199105  \n",
       "75%       0.406839     0.712596     0.712596     0.844414     0.844417  \n",
       "max       7.644546     6.539759     6.539759     3.235443     3.235442  \n",
       "\n",
       "[8 rows x 100 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displays the summary statistics of scaled cross validation features\n",
    "pd.DataFrame(cross_val_set).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df0d7910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.036730</td>\n",
       "      <td>0.004585</td>\n",
       "      <td>-0.022179</td>\n",
       "      <td>-0.037445</td>\n",
       "      <td>-0.018666</td>\n",
       "      <td>-0.031552</td>\n",
       "      <td>-0.019915</td>\n",
       "      <td>0.023039</td>\n",
       "      <td>0.032714</td>\n",
       "      <td>0.012936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048962</td>\n",
       "      <td>0.049046</td>\n",
       "      <td>0.049125</td>\n",
       "      <td>0.049200</td>\n",
       "      <td>0.049270</td>\n",
       "      <td>0.049337</td>\n",
       "      <td>0.026197</td>\n",
       "      <td>0.026197</td>\n",
       "      <td>0.029484</td>\n",
       "      <td>0.029484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.989699</td>\n",
       "      <td>1.005296</td>\n",
       "      <td>0.996918</td>\n",
       "      <td>0.973213</td>\n",
       "      <td>0.997480</td>\n",
       "      <td>0.984266</td>\n",
       "      <td>0.991701</td>\n",
       "      <td>0.994856</td>\n",
       "      <td>0.986121</td>\n",
       "      <td>1.039369</td>\n",
       "      <td>...</td>\n",
       "      <td>1.066237</td>\n",
       "      <td>1.066331</td>\n",
       "      <td>1.066418</td>\n",
       "      <td>1.066500</td>\n",
       "      <td>1.066579</td>\n",
       "      <td>1.066655</td>\n",
       "      <td>0.999034</td>\n",
       "      <td>0.999034</td>\n",
       "      <td>0.994990</td>\n",
       "      <td>0.994990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.306937</td>\n",
       "      <td>-0.393064</td>\n",
       "      <td>-1.029144</td>\n",
       "      <td>-0.629526</td>\n",
       "      <td>-1.740117</td>\n",
       "      <td>-0.545771</td>\n",
       "      <td>-0.464768</td>\n",
       "      <td>-2.393763</td>\n",
       "      <td>-2.498009</td>\n",
       "      <td>-1.091691</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.878411</td>\n",
       "      <td>-0.877077</td>\n",
       "      <td>-0.875851</td>\n",
       "      <td>-0.874563</td>\n",
       "      <td>-0.873170</td>\n",
       "      <td>-0.871902</td>\n",
       "      <td>-1.505667</td>\n",
       "      <td>-1.505667</td>\n",
       "      <td>-1.908322</td>\n",
       "      <td>-1.908323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.306937</td>\n",
       "      <td>-0.393064</td>\n",
       "      <td>-0.779574</td>\n",
       "      <td>-0.629526</td>\n",
       "      <td>-0.750446</td>\n",
       "      <td>-0.545771</td>\n",
       "      <td>-0.464768</td>\n",
       "      <td>-0.954003</td>\n",
       "      <td>-0.882737</td>\n",
       "      <td>-0.561985</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.757335</td>\n",
       "      <td>-0.755931</td>\n",
       "      <td>-0.755405</td>\n",
       "      <td>-0.754554</td>\n",
       "      <td>-0.755290</td>\n",
       "      <td>-0.754741</td>\n",
       "      <td>-0.988874</td>\n",
       "      <td>-0.988874</td>\n",
       "      <td>-0.932069</td>\n",
       "      <td>-0.932068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.765148</td>\n",
       "      <td>-0.393064</td>\n",
       "      <td>-0.592397</td>\n",
       "      <td>-0.629526</td>\n",
       "      <td>-0.509716</td>\n",
       "      <td>-0.545771</td>\n",
       "      <td>-0.464768</td>\n",
       "      <td>0.620168</td>\n",
       "      <td>0.608359</td>\n",
       "      <td>-0.475059</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.336062</td>\n",
       "      <td>-0.337044</td>\n",
       "      <td>-0.337022</td>\n",
       "      <td>-0.342636</td>\n",
       "      <td>-0.345143</td>\n",
       "      <td>-0.347967</td>\n",
       "      <td>0.180694</td>\n",
       "      <td>0.180694</td>\n",
       "      <td>0.237223</td>\n",
       "      <td>0.237220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.765148</td>\n",
       "      <td>-0.393064</td>\n",
       "      <td>1.029804</td>\n",
       "      <td>0.353356</td>\n",
       "      <td>0.934668</td>\n",
       "      <td>0.030509</td>\n",
       "      <td>-0.464768</td>\n",
       "      <td>0.696693</td>\n",
       "      <td>0.727608</td>\n",
       "      <td>0.114409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.478813</td>\n",
       "      <td>0.479584</td>\n",
       "      <td>0.479741</td>\n",
       "      <td>0.480213</td>\n",
       "      <td>0.480683</td>\n",
       "      <td>0.481176</td>\n",
       "      <td>0.704620</td>\n",
       "      <td>0.704620</td>\n",
       "      <td>0.833459</td>\n",
       "      <td>0.833457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.765148</td>\n",
       "      <td>2.544116</td>\n",
       "      <td>2.215259</td>\n",
       "      <td>3.192793</td>\n",
       "      <td>2.512792</td>\n",
       "      <td>4.448657</td>\n",
       "      <td>4.963870</td>\n",
       "      <td>1.420743</td>\n",
       "      <td>1.944728</td>\n",
       "      <td>4.232535</td>\n",
       "      <td>...</td>\n",
       "      <td>5.815940</td>\n",
       "      <td>5.815961</td>\n",
       "      <td>5.815899</td>\n",
       "      <td>5.815784</td>\n",
       "      <td>5.815587</td>\n",
       "      <td>5.815307</td>\n",
       "      <td>6.550388</td>\n",
       "      <td>6.550388</td>\n",
       "      <td>3.240708</td>\n",
       "      <td>3.240714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3            4   \\\n",
       "count  1300.000000  1300.000000  1300.000000  1300.000000  1300.000000   \n",
       "mean      0.036730     0.004585    -0.022179    -0.037445    -0.018666   \n",
       "std       0.989699     1.005296     0.996918     0.973213     0.997480   \n",
       "min      -1.306937    -0.393064    -1.029144    -0.629526    -1.740117   \n",
       "25%      -1.306937    -0.393064    -0.779574    -0.629526    -0.750446   \n",
       "50%       0.765148    -0.393064    -0.592397    -0.629526    -0.509716   \n",
       "75%       0.765148    -0.393064     1.029804     0.353356     0.934668   \n",
       "max       0.765148     2.544116     2.215259     3.192793     2.512792   \n",
       "\n",
       "                5            6            7            8            9   ...  \\\n",
       "count  1300.000000  1300.000000  1300.000000  1300.000000  1300.000000  ...   \n",
       "mean     -0.031552    -0.019915     0.023039     0.032714     0.012936  ...   \n",
       "std       0.984266     0.991701     0.994856     0.986121     1.039369  ...   \n",
       "min      -0.545771    -0.464768    -2.393763    -2.498009    -1.091691  ...   \n",
       "25%      -0.545771    -0.464768    -0.954003    -0.882737    -0.561985  ...   \n",
       "50%      -0.545771    -0.464768     0.620168     0.608359    -0.475059  ...   \n",
       "75%       0.030509    -0.464768     0.696693     0.727608     0.114409  ...   \n",
       "max       4.448657     4.963870     1.420743     1.944728     4.232535  ...   \n",
       "\n",
       "                90           91           92           93           94  \\\n",
       "count  1300.000000  1300.000000  1300.000000  1300.000000  1300.000000   \n",
       "mean      0.048962     0.049046     0.049125     0.049200     0.049270   \n",
       "std       1.066237     1.066331     1.066418     1.066500     1.066579   \n",
       "min      -0.878411    -0.877077    -0.875851    -0.874563    -0.873170   \n",
       "25%      -0.757335    -0.755931    -0.755405    -0.754554    -0.755290   \n",
       "50%      -0.336062    -0.337044    -0.337022    -0.342636    -0.345143   \n",
       "75%       0.478813     0.479584     0.479741     0.480213     0.480683   \n",
       "max       5.815940     5.815961     5.815899     5.815784     5.815587   \n",
       "\n",
       "                95           96           97           98           99  \n",
       "count  1300.000000  1300.000000  1300.000000  1300.000000  1300.000000  \n",
       "mean      0.049337     0.026197     0.026197     0.029484     0.029484  \n",
       "std       1.066655     0.999034     0.999034     0.994990     0.994990  \n",
       "min      -0.871902    -1.505667    -1.505667    -1.908322    -1.908323  \n",
       "25%      -0.754741    -0.988874    -0.988874    -0.932069    -0.932068  \n",
       "50%      -0.347967     0.180694     0.180694     0.237223     0.237220  \n",
       "75%       0.481176     0.704620     0.704620     0.833459     0.833457  \n",
       "max       5.815307     6.550388     6.550388     3.240708     3.240714  \n",
       "\n",
       "[8 rows x 100 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displays the summary statistics of scaled testing features\n",
    "pd.DataFrame(test_set).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf8b09f",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "### Logistic Regression Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9caafe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=200)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defines the logistic regression model\n",
    "log_regress_model = lr(max_iter=200 )    # default max_iter is 100, which is insufficient due to large number of training examples\n",
    "\n",
    "# Calls the defined model to fit our training data\n",
    "log_regress_model.fit(train_set, train_labels.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f4a87c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9942857142857143"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluates the performance of our model\n",
    "log_regress_model.score(train_set, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831c8767",
   "metadata": {},
   "source": [
    "> From the score of 0.99 obtained, it can be deduced that the trained model performs fairly well on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c0d1b5",
   "metadata": {},
   "source": [
    "<a id=\"cross\"></a>\n",
    "### Model Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7f04d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicts the labels for the cross validation dataset\n",
    "pred_cross_val_labels = log_regress_model.predict(cross_val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f7bfcc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 1.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_cross_val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "746c077a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9938461538461538"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluates the performance of the model on the cross validation dataset \n",
    "log_regress_model.score(cross_val_set,cross_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "faf575ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.99      1.00       890\n",
      "         1.0       0.99      0.99      0.99       410\n",
      "\n",
      "    accuracy                           0.99      1300\n",
      "   macro avg       0.99      0.99      0.99      1300\n",
      "weighted avg       0.99      0.99      0.99      1300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checks the model performance by comparing the actual labels against the predicted labels using other evaluation metrics\n",
    "print(classification_report(cross_val_labels, pred_cross_val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bd7fd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a fuction to plot a heatmap of  actual labels against predicted labels when called\n",
    "def heatmap(actual_label,pred_label,heat_map_title):\n",
    "    \n",
    "    cm = confusion_matrix(actual_label, pred_label)     # Computes the confusion matrix\n",
    "    ax = sns.heatmap(cm, linewidths=2, linecolor='black',  annot=True, fmt=\"d\", cmap=\"Greens\") # Plots the heatmap using confusion matrix values\n",
    "    \n",
    "    ax.set_title(f\"{heat_map_title}\", fontsize =12) # defines the title of the plot\n",
    "    plt.xlabel('Predicted Label', fontsize = 12)    # x-axis label with fontsize 15\n",
    "    plt.ylabel('Actual Label', fontsize = 12)       # y-axis label with fontsize 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "818ac181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calls the heatmap function defined earlier for the cross validation results\n",
    "heatmap(cross_val_labels, pred_cross_val_labels, \"Cross Validation: Actual VS Predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1039d1",
   "metadata": {},
   "source": [
    "> The score obtained suggests that the performance of the model on the cross-validation dataset is adequate as well. However, for large datasets containing an imbalanced number of classes it has been shown in previous studies that single-value evaluation metrics may be susceptible to errors (Juba and Le 2019). <br><br>Given the imbalanced nature of our dataset containing 4648 (68%) noise and 2152 (30%) microearthquakes. It was therefore vital to assess the performance of the model with the use of other evaluation metrics such as precision-recall. Nonetheless, results from the other evaluation metrics confirm the good performance of the trained logistic regression model.<br><br>\n",
    "The heatmap shows the misclassification between the actual labels and the predicted the cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d6f454",
   "metadata": {},
   "source": [
    "<a id=\"test\"></a>\n",
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5480a1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicts the labels for the test dataset\n",
    "pred_test_labels = log_regress_model.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e30a4e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ffd58b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9984615384615385"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluates the performance of the model on the test dataset \n",
    "log_regress_model.score(test_set,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d89c3949",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       904\n",
      "         1.0       1.00      1.00      1.00       396\n",
      "\n",
      "    accuracy                           1.00      1300\n",
      "   macro avg       1.00      1.00      1.00      1300\n",
      "weighted avg       1.00      1.00      1.00      1300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the model performance by comparing the actual labels against the predicted labels\n",
    "print(classification_report(test_labels, pred_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "258f1f49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calls the heatmap function defined earlier for the test results\n",
    "heatmap(test_labels, pred_test_labels, \"Test: Actual VS Predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0528e7",
   "metadata": {},
   "source": [
    "> Results obtained from the testing proves that the trained logistic regression model performs well on both the cross-validation and testing datasets, with a slightly better perfomance for the testing dataset compared to the corss-validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96889d3",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9b2038",
   "metadata": {},
   "source": [
    "> *Objective 1:* Data exploration and preprocessing were conducted on the provided dataset relying primarily on statistical information obtained about the data. Armed by this information, feature scaling was performed ensuring a better gradient descent convergence.<br>\n",
    "*Objective 2:* The decent logistic regression model was adequately trained to facilitate the prediction of microearthquakes using microearthquake data.<br>\n",
    "*Objective 3:* Although perfect accuracies were not achieved for either the cross-validation or the testing dataset, results for both cases, as deduced from the different evaluation metrics conducted on the cross-validation and testing datasets, all indicate that the trained logistic regression model performs well if provided with unseen data points.<br>\n",
    "*Objective 4:* To implement the utilization of the tanh function in place of the sigmoid function, the first adjustment required would be to adjust the original labels used in training, validating, and testing the model to map from (0,1) to (-1,1), where -1 would represent noise and 1 would represent microearthquakes. This adjustment would be crucial to training the model because the tanh function maps to a range of (-1,1) as against the sigmoid function which maps to the probability range of (0,1).<br>\n",
    "Following the adjustment of the labels, to implement this classification in python, the source code for the classification algorithm could either be written from scratch or the use of any python library which offers such functionality could be exploited. It is, however, crucial to point out that most current python libraries provide the use of the tanh function within the framework of neural networks. Therefore in using these libraries, we may be required to build the model under the framework of a neural network, albeit our network would be one consisting of a single neuron and no hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdd3bbf",
   "metadata": {},
   "source": [
    "<a id=\"reference\"></a>\n",
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119f1a2c",
   "metadata": {},
   "source": [
    "- Alshaher, H., 2021. Studying the effects of feature scaling in machine learning (Doctoral dissertation, North Carolina Agricultural and Technical State University).\n",
    "- Bisong, E. and Bisong, E., 2019. Optimization for machine learning: Gradient descent. Building Machine Learning and Deep Learning Models on Google Cloud Platform: A Comprehensive Guide for Beginners, pp.203-207.\n",
    "- Gasso, G., 2019. Logistic regression. INSA Rouen-ASI Departement Laboratory: Saint-Etienne-du-Rouvray, France, pp.1-30.\n",
    "- Juba, B. and Le, H.S., 2019. Precision-recall versus accuracy and the role of large data sets. In Proceedings of the AAAI conference on artificial intelligence (Vol. 33, No. 01, pp. 4039-4048).\n",
    "- Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V. and Vanderplas, J., 2011. Scikit-learn: Machine learning in Python. the Journal of machine Learning research, 12, pp.2825-2830.\n",
    "- Reitermanova, Z., 2010, June. Data splitting. In WDS (Vol. 10, pp. 31-36). Prague: Matfyzpress."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
